

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>6.1 Schedule with Airflow &mdash; Cloud Data Stack  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=0e30c378" />

  
    <link rel="shortcut icon" href="../../_static/logo_white_16x16.png"/>
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="7. Governance" href="../07_governance.html" />
    <link rel="prev" title="6. Orchestration" href="../06_orchestration.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >

          
          
          <a href="../../index.html">
            
              <img src="../../_static/logo_text_white.svg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Services:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../services/01_enterprise_data_platforms.html">1. Enterprise Data Platforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../services/02_ai_workflows.html">2. AI Workflows</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../services/03_data_migrations.html">3. Data Migrations</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guides:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../guides/01_analytics_engineering.html">1. Analytics Engineering</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Technology Stack:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../01_extraction.html">1. Extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_streaming.html">2. Streaming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_storage.html">3. Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../04_compute.html">4. Compute and Query</a></li>
<li class="toctree-l1"><a class="reference internal" href="../05_transformation.html">5. Transformation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../06_orchestration.html">6. Orchestration</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">6.1 Schedule with Airflow</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#core-requirements-design-considerations">Core Requirements &amp; Design Considerations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#recommended-project-structure">Recommended Project Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="#python-project-setup-with-uv">Python Project Setup with <code class="docutils literal notranslate"><span class="pre">uv</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#configuring-for-aws-mwaa">Configuring for AWS MWAA</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scheduling-databricks-workloads">Scheduling Databricks Workloads</a></li>
<li class="toctree-l3"><a class="reference internal" href="#local-development-with-docker">Local Development with Docker</a></li>
<li class="toctree-l3"><a class="reference internal" href="#managing-environments-test-vs-prod">Managing Environments (Test vs. Prod)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#potential-tradeoffs-and-considerations">Potential Tradeoffs and Considerations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-common-concerns">Other Common Concerns</a></li>
<li class="toctree-l3"><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../07_governance.html">7. Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../08_ai_workflows.html">8. AI Workflows</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #343131" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Cloud Data Stack</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../06_orchestration.html">6. Orchestration</a></li>
      <li class="breadcrumb-item active">6.1 Schedule with Airflow</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="schedule-with-airflow">
<h1>6.1 Schedule with Airflow<a class="headerlink" href="#schedule-with-airflow" title="Link to this heading"></a></h1>
<p>How to schedule your Workloads on Databricks with Airflow.</p>
<nav class="contents local" id="article-outline">
<p class="topic-title">Article Outline</p>
<ul class="simple">
<li><p><a class="reference internal" href="#introduction" id="id2">Introduction</a></p></li>
<li><p><a class="reference internal" href="#core-requirements-design-considerations" id="id3">Core Requirements &amp; Design Considerations</a></p></li>
<li><p><a class="reference internal" href="#recommended-project-structure" id="id4">Recommended Project Structure</a></p></li>
<li><p><a class="reference internal" href="#python-project-setup-with-uv" id="id5">Python Project Setup with <code class="docutils literal notranslate"><span class="pre">uv</span></code></a></p></li>
<li><p><a class="reference internal" href="#configuring-for-aws-mwaa" id="id6">Configuring for AWS MWAA</a></p></li>
<li><p><a class="reference internal" href="#scheduling-databricks-workloads" id="id7">Scheduling Databricks Workloads</a></p></li>
<li><p><a class="reference internal" href="#local-development-with-docker" id="id8">Local Development with Docker</a></p></li>
<li><p><a class="reference internal" href="#managing-environments-test-vs-prod" id="id9">Managing Environments (Test vs. Prod)</a></p></li>
<li><p><a class="reference internal" href="#potential-tradeoffs-and-considerations" id="id10">Potential Tradeoffs and Considerations</a></p></li>
<li><p><a class="reference internal" href="#other-common-concerns" id="id11">Other Common Concerns</a></p></li>
<li><p><a class="reference internal" href="#conclusion" id="id12">Conclusion</a></p></li>
<li><p><a class="reference internal" href="#references" id="id13">References</a></p></li>
</ul>
</nav>
<section id="introduction">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Introduction</a><a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Brief overview of using Airflow, specifically AWS Managed Workflows for Apache Airflow (MWAA), to orchestrate workloads on Databricks.</p></li>
<li><p>Benefits: robust scheduling, dependency management, visibility, and centralized workflow management.</p></li>
<li><p>Article Goal: Guide readers through setting up a production-ready Airflow project for Databricks, emphasizing:</p>
<ul>
<li><p>AWS MWAA for deployment.</p></li>
<li><p>Python project setup with the <code class="docutils literal notranslate"><span class="pre">uv</span></code> package manager.</p></li>
<li><p>Scheduling dbt SQL models, Databricks notebooks, and general Python/Bash DAGs.</p></li>
<li><p>Local development using Docker.</p></li>
<li><p>Managing test and production environments.</p></li>
</ul>
</li>
<li><p>Reference to a Template Project: This article uses examples and principles from the <a class="reference external" href="https://github.com/clouddatastack/aws-airflow-databricks">clouddatastack/aws-airflow-databricks</a> template project.</p></li>
</ul>
</section>
<section id="core-requirements-design-considerations">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Core Requirements &amp; Design Considerations</a><a class="headerlink" href="#core-requirements-design-considerations" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Workload Diversity</strong>: Effectively scheduling and managing:</p>
<ul>
<li><p>dbt Core SQL models.</p></li>
<li><p>Databricks notebooks.</p></li>
<li><p>General purpose Airflow DAGs (e.g., Python scripts, Bash commands).</p></li>
</ul>
</li>
<li><p><strong>Environment Parity</strong>:</p>
<ul>
<li><p>Local Development: Airflow running in Docker, connecting to a test Databricks workspace.</p></li>
<li><p>Test Environment: Airflow (MWAA or local) targeting a dedicated test Databricks workspace.</p></li>
<li><p>Production Environment: AWS MWAA targeting the production Databricks workspace.</p></li>
</ul>
</li>
<li><p><strong>Package Management</strong>: Utilizing <code class="docutils literal notranslate"><span class="pre">uv</span></code> for fast and efficient Python dependency management.</p></li>
<li><p><strong>Deployment</strong>: Specifics of deploying to and configuring AWS MWAA.</p></li>
<li><p><strong>Configuration Management</strong>: Handling connections, variables, and configurations across different environments.</p></li>
</ul>
</section>
<section id="recommended-project-structure">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Recommended Project Structure</a><a class="headerlink" href="#recommended-project-structure" title="Link to this heading"></a></h2>
<p>A well-organized project structure is crucial for maintainability and scalability.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>airflow-databricks-project/
├── dags/                     # Airflow DAGs
│   ├── common/               # Common utilities, custom operators, hooks
│   │   ├── __init__.py
│   │   └── utils.py          # Placeholder for shared utilities
│   ├── dbt_runs/             # DAGs for running dbt tasks
│   │   ├── __init__.py
│   │   └── dag_dbt_daily_models.py
│   ├── notebook_jobs/        # DAGs for running Databricks notebooks
│   │   ├── __init__.py
│   │   └── dag_notebook_etl.py
│   └── generic_tasks/        # General purpose DAGs
│       ├── __init__.py
│       └── dag_generic_data_pipeline.py
├── plugins/                  # Custom Airflow plugins (if any)
│   └── __init__.py
├── dbt_project/              # Your dbt project
│   ├── dbt_project.yml
│   ├── models/
│   └── ...
├── notebooks_source/         # Source Databricks notebooks
│   └── etl_notebook.py       # Example ETL notebook
├── requirements/             # Python dependencies
│   ├── base.in               # Abstract dependencies for uv compile (e.g., apache-airflow, apache-airflow-providers-databricks)
│   ├── base.txt              # Pinned dependencies for Airflow (local &amp; MWAA)
│   ├── dev.in                # Development specific dependencies
│   └── dev.txt               # Pinned dev dependencies
├── tests/                    # Automated tests
│   ├── dags/                 # DAG integrity and unit tests
│   └── operators/            # Tests for custom operators
├── scripts/                  # Helper scripts (e.g., deployment, local env setup)
│   └── setup_local_env.sh    # Example script for local setup
├── Dockerfile                # For local Airflow development environment
├── docker-compose.yml        # For orchestrating local Airflow services
├── .env.example              # Example environment variables
└── README.md
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Rationale</strong>:</p>
<ul>
<li><p><cite>dags/</cite>: Clear separation of DAGs by purpose (dbt, notebooks, generic).</p></li>
<li><p><cite>dags/common/</cite>: Promotes DRY principles by centralizing shared code.</p></li>
<li><p><cite>requirements/</cite>: Manages dependencies clearly for different contexts using <code class="docutils literal notranslate"><span class="pre">uv</span></code> for generation.</p></li>
<li><p><cite>notebooks_source/</cite>: Keeps source notebooks version-controlled; these would be deployed to Databricks (e.g., via DBFS or Databricks Repos).</p></li>
</ul>
</li>
</ul>
</section>
<section id="python-project-setup-with-uv">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Python Project Setup with <code class="docutils literal notranslate"><span class="pre">uv</span></code></a><a class="headerlink" href="#python-project-setup-with-uv" title="Link to this heading"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">uv</span></code> is a fast Python package installer and resolver.</p>
<ul>
<li><p><strong>Virtual Environment with ``uv``</strong>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a virtual environment</span>
uv<span class="w"> </span>venv
<span class="nb">source</span><span class="w"> </span>.venv/bin/activate
</pre></div>
</div>
</li>
<li><p><strong>Managing Dependencies</strong>:</p>
<ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">*.in</span></code> files for abstract dependencies and <code class="docutils literal notranslate"><span class="pre">uv</span> <span class="pre">pip</span> <span class="pre">compile</span></code> to generate pinned <code class="docutils literal notranslate"><span class="pre">*.txt</span></code> files.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compile base requirements</span>
uv<span class="w"> </span>pip<span class="w"> </span>compile<span class="w"> </span>requirements/base.in<span class="w"> </span>-o<span class="w"> </span>requirements/base.txt

<span class="c1"># Install requirements</span>
uv<span class="w"> </span>pip<span class="w"> </span>sync<span class="w"> </span>requirements/base.txt
</pre></div>
</div>
</li>
<li><p><strong>Integration with Docker</strong>: The <code class="docutils literal notranslate"><span class="pre">Dockerfile</span></code> will use the generated <code class="docutils literal notranslate"><span class="pre">requirements/base.txt</span></code> (from <code class="docutils literal notranslate"><span class="pre">requirements/base.in</span></code>) for the Airflow image. The template project's <code class="docutils literal notranslate"><span class="pre">Dockerfile</span></code> demonstrates this:</p>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span><span class="c"># Start from a Python base image to use uv fully</span>
<span class="k">FROM</span><span class="w"> </span><span class="s">python:3.10-slim</span>

<span class="k">ENV</span><span class="w"> </span><span class="nv">AIRFLOW_HOME</span><span class="o">=</span>/opt/airflow
<span class="k">ENV</span><span class="w"> </span><span class="nv">AIRFLOW__CORE__DAGS_FOLDER</span><span class="o">=</span><span class="si">${</span><span class="nv">AIRFLOW_HOME</span><span class="si">}</span>/dags
<span class="k">ENV</span><span class="w"> </span><span class="nv">AIRFLOW__CORE__LOAD_EXAMPLES</span><span class="o">=</span>False
<span class="k">ENV</span><span class="w"> </span><span class="nv">AIRFLOW__CORE__EXECUTOR</span><span class="o">=</span>LocalExecutor

<span class="c"># Install uv</span>
<span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-cache-dir<span class="w"> </span>uv

<span class="k">WORKDIR</span><span class="w"> </span><span class="s">$AIRFLOW_HOME</span>
<span class="k">COPY</span><span class="w"> </span>requirements/base.txt<span class="w"> </span>.
<span class="c"># Install Python dependencies using uv</span>
<span class="k">RUN</span><span class="w"> </span>uv<span class="w"> </span>pip<span class="w"> </span>sync<span class="w"> </span>base.txt<span class="w"> </span>--system<span class="w"> </span>--no-cache

<span class="k">COPY</span><span class="w"> </span>dags/<span class="w"> </span>./dags/
<span class="k">COPY</span><span class="w"> </span>plugins/<span class="w"> </span>./plugins/

<span class="k">EXPOSE</span><span class="w"> </span><span class="s">8080</span>
<span class="k">CMD</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;airflow&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;standalone&quot;</span><span class="p">]</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="configuring-for-aws-mwaa">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Configuring for AWS MWAA</a><a class="headerlink" href="#configuring-for-aws-mwaa" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>S3 Bucket</strong>: MWAA requires an S3 bucket for DAGs, plugins, and the <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> file.</p></li>
<li><p><strong>`requirements.txt` for MWAA</strong>:</p>
<ul>
<li><p>Upload your <code class="docutils literal notranslate"><span class="pre">requirements/base.txt</span></code> (compiled by <code class="docutils literal notranslate"><span class="pre">uv</span></code> from <code class="docutils literal notranslate"><span class="pre">requirements/base.in</span></code>) to the S3 bucket.</p></li>
<li><p>Ensure it includes necessary providers, e.g., <code class="docutils literal notranslate"><span class="pre">apache-airflow-providers-databricks</span></code>, <code class="docutils literal notranslate"><span class="pre">apache-airflow&gt;=2.8.0</span></code> (or your target version).</p></li>
</ul>
</li>
<li><p><strong>IAM Roles &amp; Permissions</strong>:</p>
<ul>
<li><p>MWAA Execution Role: Needs permissions to access S3, CloudWatch Logs, and to assume roles for accessing other services like Databricks.</p></li>
<li><p>Databricks Access: Configure Databricks connection in Airflow using a Databricks personal access token (PAT) or Azure AD service principal, stored securely (e.g., AWS Secrets Manager and referenced in Airflow connection).</p></li>
</ul>
</li>
<li><p><strong>Environment Variables in MWAA</strong>: For Databricks host, tokens, cluster IDs, etc.</p></li>
<li><p><em>Reference</em>: <a class="reference external" href="https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html">AWS MWAA User Guide</a></p></li>
</ul>
</section>
<section id="scheduling-databricks-workloads">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Scheduling Databricks Workloads</a><a class="headerlink" href="#scheduling-databricks-workloads" title="Link to this heading"></a></h2>
<ol class="arabic">
<li><p><strong>Databricks Connection in Airflow</strong>:</p>
<ul>
<li><p>Create a Databricks connection in the Airflow UI or via environment variables.</p></li>
<li><p>Key fields: <code class="docutils literal notranslate"><span class="pre">databricks_conn_id</span></code> (e.g., <code class="docutils literal notranslate"><span class="pre">databricks_default</span></code>), <code class="docutils literal notranslate"><span class="pre">host</span></code>, <code class="docutils literal notranslate"><span class="pre">token</span></code> (or other auth methods like Azure Service Principal).</p></li>
<li><p><em>Code Example (Environment Variable for Connection in ``docker-compose.yml`` or MWAA)</em>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">AIRFLOW_CONN_DATABRICKS_DEFAULT</span><span class="o">=</span><span class="se">\\\&#39;</span><span class="o">{</span><span class="se">\\</span>n<span class="se">\\</span>
<span class="w">    </span><span class="s2">&quot;conn_type&quot;</span>:<span class="w"> </span><span class="s2">&quot;databricks&quot;</span>,<span class="se">\\</span>n<span class="se">\\</span>
<span class="w">    </span><span class="s2">&quot;host&quot;</span>:<span class="w"> </span><span class="s2">&quot;https://your-databricks-instance.azuredatabricks.net&quot;</span>,<span class="se">\\</span>n<span class="se">\\</span>
<span class="w">    </span><span class="s2">&quot;token&quot;</span>:<span class="w"> </span><span class="s2">&quot;dapiXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;</span>,<span class="se">\\</span>n<span class="se">\\</span>
<span class="w">    </span><span class="s2">&quot;extra&quot;</span>:<span class="w"> </span><span class="o">{</span><span class="s2">&quot;job_cluster_name_prefix&quot;</span>:<span class="w"> </span><span class="s2">&quot;airflow-local-&quot;</span><span class="o">}</span><span class="se">\\</span>n<span class="se">\\</span>
<span class="o">}</span><span class="se">\\\&#39;</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p><strong>Scheduling Databricks Notebooks</strong>:</p>
<ul>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">DatabricksSubmitRunOperator</span></code> for submitting notebook tasks as new jobs.</p></li>
<li><p><em>Code Example (from ``dags/notebook_jobs/dag_notebook_etl.py``)</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">airflow.models.dag</span><span class="w"> </span><span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">airflow_providers_databricks.operators.databricks</span><span class="w"> </span><span class="kn">import</span> <span class="n">DatabricksSubmitRunOperator</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pendulum</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
    <span class="n">dag_id</span><span class="o">=</span>\\\<span class="s1">&#39;databricks_notebook_etl_example</span><span class="se">\\\&#39;</span><span class="s1">,</span>
    <span class="n">start_date</span><span class="o">=</span><span class="n">pendulum</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2023</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tz</span><span class="o">=</span><span class="s2">&quot;UTC&quot;</span><span class="p">),</span>
    <span class="n">schedule_interval</span><span class="o">=</span>\\\<span class="s1">&#39;@daily</span><span class="se">\\\&#39;</span><span class="s1">,</span>
    <span class="n">catchup</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span>\\\<span class="s1">&#39;databricks</span><span class="se">\\\&#39;</span><span class="s1">, </span><span class="se">\\\&#39;</span><span class="s1">notebook</span><span class="se">\\\&#39;</span><span class="s1">],</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>
    <span class="n">submit_notebook_task</span> <span class="o">=</span> <span class="n">DatabricksSubmitRunOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span>\\\<span class="s1">&#39;run_etl_notebook</span><span class="se">\\\&#39;</span><span class="s1">,</span>
        <span class="n">databricks_conn_id</span><span class="o">=</span>\\\<span class="s1">&#39;databricks_default</span><span class="se">\\\&#39;</span><span class="s1">,</span>
        <span class="n">new_cluster</span><span class="o">=</span><span class="p">{</span>
            \\\<span class="s1">&#39;spark_version</span><span class="se">\\\&#39;</span><span class="s1">: </span><span class="se">\\\&#39;</span><span class="s1">13.3.x-scala2.12</span><span class="se">\\\&#39;</span><span class="s1">, # Or your desired Spark version</span>
            \\\<span class="s1">&#39;node_type_id</span><span class="se">\\\&#39;</span><span class="s1">: </span><span class="se">\\\&#39;</span><span class="s1">i3.xlarge</span><span class="se">\\\&#39;</span><span class="s1">,    # Choose appropriate instance type</span>
            \\\<span class="s1">&#39;num_workers</span><span class="se">\\\&#39;</span><span class="s1">: 2,</span>
        <span class="p">},</span>
        <span class="n">notebook_task</span><span class="o">=</span><span class="p">{</span>
            \\\<span class="s1">&#39;notebook_path</span><span class="se">\\\&#39;</span><span class="s1">: </span><span class="se">\\\&#39;</span><span class="s1">/Shared/airflow_notebooks/etl_notebook.py</span><span class="se">\\\&#39;</span><span class="s1">, # Adjust path in Databricks</span>
            \\\<span class="s1">&#39;base_parameters</span><span class="se">\\\&#39;</span><span class="s1">: {</span><span class="se">\\\&#39;</span><span class="s1">param1</span><span class="se">\\\&#39;</span><span class="s1">: </span><span class="se">\\\&#39;</span><span class="s1">value_from_airflow</span><span class="se">\\\&#39;</span><span class="s1">, </span><span class="se">\\\&#39;</span><span class="s1">date</span><span class="se">\\\&#39;</span><span class="s1">: </span><span class="se">\\\&#39;</span><span class="s1">{{ ds }}</span><span class="se">\\\&#39;</span><span class="s1">}</span>
        <span class="p">},</span>
    <span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p><strong>Scheduling dbt Workloads</strong>:</p>
<ul>
<li><p><strong>Strategy</strong>: The template project uses <code class="docutils literal notranslate"><span class="pre">DatabricksSubmitRunOperator</span></code> with a <code class="docutils literal notranslate"><span class="pre">spark_python_task</span></code> that points to a Python script on DBFS. This script is responsible for invoking dbt CLI commands.</p></li>
<li><p>This requires:</p>
<ul class="simple">
<li><p>Your dbt project to be accessible by the Databricks job (e.g., synced via Databricks Repos, or copied to DBFS).</p></li>
<li><p>A Python script (e.g., <code class="docutils literal notranslate"><span class="pre">dbt_runner_script.py</span></code>) on DBFS that can execute dbt commands.</p></li>
<li><p>The Databricks cluster (either new or existing) must have dbt installed (e.g., via init scripts or by using a custom Docker container for the cluster).</p></li>
</ul>
</li>
<li><p><em>Code Example (from ``dags/dbt_runs/dag_dbt_daily_models.py``)</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">airflow.models.dag</span><span class="w"> </span><span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">airflow_providers_databricks.operators.databricks</span><span class="w"> </span><span class="kn">import</span> <span class="n">DatabricksSubmitRunOperator</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pendulum</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
    <span class="n">dag_id</span><span class="o">=</span>\\\<span class="s1">&#39;dbt_daily_models_example</span><span class="se">\\\&#39;</span><span class="s1">,</span>
    <span class="n">start_date</span><span class="o">=</span><span class="n">pendulum</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2023</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tz</span><span class="o">=</span><span class="s2">&quot;UTC&quot;</span><span class="p">),</span>
    <span class="n">schedule_interval</span><span class="o">=</span>\\\<span class="s1">&#39;@daily</span><span class="se">\\\&#39;</span><span class="s1">,</span>
    <span class="n">catchup</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span>\\\<span class="s1">&#39;dbt</span><span class="se">\\\&#39;</span><span class="s1">, </span><span class="se">\\\&#39;</span><span class="s1">databricks</span><span class="se">\\\&#39;</span><span class="s1">],</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>
    <span class="n">dbt_run_models</span> <span class="o">=</span> <span class="n">DatabricksSubmitRunOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span>\\\<span class="s1">&#39;dbt_run_daily_models</span><span class="se">\\\&#39;</span><span class="s1">,</span>
        <span class="n">databricks_conn_id</span><span class="o">=</span>\\\<span class="s1">&#39;databricks_default</span><span class="se">\\\&#39;</span><span class="s1">,</span>
        <span class="n">new_cluster</span><span class="o">=</span><span class="p">{</span>
            \\\<span class="s1">&#39;spark_version</span><span class="se">\\\&#39;</span><span class="s1">: </span><span class="se">\\\&#39;</span><span class="s1">13.3.x-scala2.12</span><span class="se">\\\&#39;</span><span class="s1">,</span>
            \\\<span class="s1">&#39;node_type_id</span><span class="se">\\\&#39;</span><span class="s1">: </span><span class="se">\\\&#39;</span><span class="s1">i3.xlarge</span><span class="se">\\\&#39;</span><span class="s1">,</span>
            \\\<span class="s1">&#39;num_workers</span><span class="se">\\\&#39;</span><span class="s1">: 1,</span>
            <span class="c1"># \\\&#39;init_scripts\\\&#39;: [ { \\\&#39;dbfs\\\&#39;: { \\\&#39;destination\\\&#39;: \\\&#39;dbfs:/FileStore/scripts/dbt_install.sh\\\&#39; } } ] # Example init script</span>
        <span class="p">},</span>
        <span class="n">spark_python_task</span><span class="o">=</span><span class="p">{</span>
            \\\<span class="s1">&#39;python_file</span><span class="se">\\\&#39;</span><span class="s1">: </span><span class="se">\\\&#39;</span><span class="s1">dbfs:/path/to/your/dbt_runner_script.py</span><span class="se">\\\&#39;</span><span class="s1">, # IMPORTANT: Create this script</span>
            \\\<span class="s1">&#39;parameters</span><span class="se">\\\&#39;</span><span class="s1">: [</span>
                \\\<span class="s1">&#39;run</span><span class="se">\\\&#39;</span><span class="s1">,</span>
                \\\<span class="s1">&#39;--models</span><span class="se">\\\&#39;</span><span class="s1">, </span><span class="se">\\\&#39;</span><span class="s1">tag:daily</span><span class="se">\\\&#39;</span><span class="s1">,</span>
                \\\<span class="s1">&#39;--project-dir</span><span class="se">\\\&#39;</span><span class="s1">, </span><span class="se">\\\&#39;</span><span class="s1">/path/to/your/dbt_project/in/repo_or_dbfs</span><span class="se">\\\&#39;</span><span class="s1">, # e.g., /dbfs/dbt_projects/my_dbt_project or /Workspace/Repos/user/my_dbt_project</span>
                \\\<span class="s1">&#39;--profiles-dir</span><span class="se">\\\&#39;</span><span class="s1">, </span><span class="se">\\\&#39;</span><span class="s1">/path/to/your/profiles_dir</span><span class="se">\\\&#39;</span><span class="s1"> # e.g., /dbfs/dbt_projects/my_dbt_project or /Workspace/Repos/user/my_dbt_project</span>
            <span class="p">]</span>
        <span class="p">}</span>
    <span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><em>Considerations for dbt runner script</em>: This script would typically use <code class="docutils literal notranslate"><span class="pre">subprocess.run()</span></code> to execute dbt commands. It needs to handle paths to the dbt project and profiles correctly within the Databricks execution environment.</p></li>
</ul>
</li>
<li><p><strong>Scheduling General Python/Bash DAGs</strong>:</p>
<ul>
<li><p>Standard Airflow operators like <code class="docutils literal notranslate"><span class="pre">PythonOperator</span></code>, <code class="docutils literal notranslate"><span class="pre">BashOperator</span></code>. These run on the Airflow worker.</p></li>
<li><p>For interactions with Databricks API from these operators: use <code class="docutils literal notranslate"><span class="pre">databricks-sdk</span></code> within a <code class="docutils literal notranslate"><span class="pre">PythonOperator</span></code>.</p></li>
<li><p><em>Code Example (from ``dags/generic_tasks/dag_generic_data_pipeline.py``)</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">airflow.models.dag</span><span class="w"> </span><span class="kn">import</span> <span class="n">DAG</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">airflow.operators.python</span><span class="w"> </span><span class="kn">import</span> <span class="n">PythonOperator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">airflow.operators.bash</span><span class="w"> </span><span class="kn">import</span> <span class="n">BashOperator</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pendulum</span>

<span class="k">def</span><span class="w"> </span><span class="nf">my_python_callable</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Running general Python task!&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="s2">&quot;Python task finished.&quot;</span>

<span class="k">def</span><span class="w"> </span><span class="nf">another_python_callable</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">ti</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span>\\\<span class="s1">&#39;ti</span><span class="se">\\\&#39;</span><span class="s1">]</span>
    <span class="n">pulled_value</span> <span class="o">=</span> <span class="n">ti</span><span class="o">.</span><span class="n">xcom_pull</span><span class="p">(</span><span class="n">task_ids</span><span class="o">=</span><span class="s2">&quot;simple_python_task&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pulled value: </span><span class="si">{</span><span class="n">pulled_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">DAG</span><span class="p">(</span>
    <span class="n">dag_id</span><span class="o">=</span>\\\<span class="s1">&#39;general_python_bash_pipeline_example</span><span class="se">\\\&#39;</span><span class="s1">,</span>
    <span class="n">start_date</span><span class="o">=</span><span class="n">pendulum</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2023</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tz</span><span class="o">=</span><span class="s2">&quot;UTC&quot;</span><span class="p">),</span>
    <span class="n">schedule</span><span class="o">=</span>\\\<span class="s1">&#39;@daily</span><span class="se">\\\&#39;</span><span class="s1">,</span>
    <span class="n">catchup</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span>\\\<span class="s1">&#39;generic</span><span class="se">\\\&#39;</span><span class="s1">, </span><span class="se">\\\&#39;</span><span class="s1">python</span><span class="se">\\\&#39;</span><span class="s1">, </span><span class="se">\\\&#39;</span><span class="s1">bash</span><span class="se">\\\&#39;</span><span class="s1">],</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">dag</span><span class="p">:</span>
    <span class="n">python_task</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span>\\\<span class="s1">&#39;simple_python_task</span><span class="se">\\\&#39;</span><span class="s1">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">my_python_callable</span>
    <span class="p">)</span>
    <span class="n">bash_task</span> <span class="o">=</span> <span class="n">BashOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span>\\\<span class="s1">&#39;simple_bash_task</span><span class="se">\\\&#39;</span><span class="s1">,</span>
        <span class="n">bash_command</span><span class="o">=</span>\\\<span class="s1">&#39;echo &quot;Running Bash command! Today is $(date)&quot;</span><span class="se">\\\&#39;</span>
    <span class="p">)</span>
    <span class="n">python_task_with_xcom</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
        <span class="n">task_id</span><span class="o">=</span>\\\<span class="s1">&#39;python_task_using_xcom</span><span class="se">\\\&#39;</span><span class="s1">,</span>
        <span class="n">python_callable</span><span class="o">=</span><span class="n">another_python_callable</span>
    <span class="p">)</span>
    <span class="n">python_task</span> <span class="o">&gt;&gt;</span> <span class="n">bash_task</span> <span class="o">&gt;&gt;</span> <span class="n">python_task_with_xcom</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ol>
</section>
<section id="local-development-with-docker">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Local Development with Docker</a><a class="headerlink" href="#local-development-with-docker" title="Link to this heading"></a></h2>
<ul>
<li><p><strong>`Dockerfile` for Airflow</strong>:</p>
<ul>
<li><p>The template project's <code class="docutils literal notranslate"><span class="pre">Dockerfile</span></code> starts from <code class="docutils literal notranslate"><span class="pre">python:3.10-slim</span></code>, installs <code class="docutils literal notranslate"><span class="pre">uv</span></code>, copies <code class="docutils literal notranslate"><span class="pre">requirements/base.txt</span></code>, and uses <code class="docutils literal notranslate"><span class="pre">uv</span> <span class="pre">pip</span> <span class="pre">sync</span> <span class="pre">base.txt</span> <span class="pre">--system</span></code> to install dependencies. It then copies DAGs and plugins.</p></li>
<li><p><em>Key `Dockerfile` instructions (refer to template for full file)</em>:</p>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span><span class="k">FROM</span><span class="w"> </span><span class="s">python:3.10-slim</span>
<span class="k">ENV</span><span class="w"> </span><span class="nv">AIRFLOW_HOME</span><span class="o">=</span>/opt/airflow
<span class="c"># ... other ENV VARS ...</span>
<span class="k">RUN</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--no-cache-dir<span class="w"> </span>uv
<span class="k">WORKDIR</span><span class="w"> </span><span class="s">$AIRFLOW_HOME</span>
<span class="k">COPY</span><span class="w"> </span>requirements/base.txt<span class="w"> </span>.
<span class="k">RUN</span><span class="w"> </span>uv<span class="w"> </span>pip<span class="w"> </span>sync<span class="w"> </span>base.txt<span class="w"> </span>--system<span class="w"> </span>--no-cache
<span class="k">COPY</span><span class="w"> </span>dags/<span class="w"> </span>./dags/
<span class="k">COPY</span><span class="w"> </span>plugins/<span class="w"> </span>./plugins/
<span class="c"># ...</span>
<span class="k">CMD</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;airflow&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;standalone&quot;</span><span class="p">]</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p><strong>`docker-compose.yml`</strong>:</p>
<ul>
<li><p>The template project's <code class="docutils literal notranslate"><span class="pre">docker-compose.yml</span></code> defines services for <code class="docutils literal notranslate"><span class="pre">postgres</span></code>, <code class="docutils literal notranslate"><span class="pre">airflow-init</span></code> (to initialize DB and create user), <code class="docutils literal notranslate"><span class="pre">airflow-webserver</span></code>, and <code class="docutils literal notranslate"><span class="pre">airflow-scheduler</span></code>.</p></li>
<li><p>It uses the local <code class="docutils literal notranslate"><span class="pre">Dockerfile</span></code> (<code class="docutils literal notranslate"><span class="pre">build:</span> <span class="pre">.</span></code>) for Airflow services.</p></li>
<li><p>Volumes are mounted for <code class="docutils literal notranslate"><span class="pre">./dags</span></code>, <code class="docutils literal notranslate"><span class="pre">./plugins</span></code>, and <code class="docutils literal notranslate"><span class="pre">./logs</span></code>.</p></li>
<li><p>Crucially, it sets environment variables, including <code class="docutils literal notranslate"><span class="pre">AIRFLOW_CONN_DATABRICKS_DEFAULT</span></code> for the local Databricks connection, and <code class="docutils literal notranslate"><span class="pre">AIRFLOW__DATABASE__SQL_ALCHEMY_CONN</span></code> for the Postgres backend.</p></li>
<li><p><em>Code Snippet (`docker-compose.yml` excerpt)</em>:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">\\\&#39;3.8\\\&#39;</span>
<span class="nt">x-airflow-common</span><span class="p">:</span><span class="w"> </span><span class="nl">&amp;airflow-common</span>
<span class="w">  </span><span class="nt">build</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">.</span><span class="w"> </span><span class="c1"># Uses the Dockerfile in the current directory</span>
<span class="w">  </span><span class="nt">environment</span><span class="p">:</span>
<span class="w">    </span><span class="nl">&amp;airflow-common-env</span>
<span class="w">    </span><span class="nt">AIRFLOW__CORE__EXECUTOR</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">LocalExecutor</span>
<span class="w">    </span><span class="nt">AIRFLOW__CORE__LOAD_EXAMPLES</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">\\\&#39;false\\\&#39;</span>
<span class="w">    </span><span class="nt">AIRFLOW__DATABASE__SQL_ALCHEMY_CONN</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">postgresql+psycopg2://airflow:airflow@postgres/airflow</span>
<span class="w">    </span><span class="nt">AIRFLOW__CORE__FERNET_KEY</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">\\\&#39;FB0o_zt3333qL9jAbELJ7z3gLh2aK3N2ENc2Ld1sL_Y=\\\&#39;</span><span class="w"> </span><span class="c1"># Replace for production</span>
<span class="w">    </span><span class="nt">AIRFLOW_CONN_DATABRICKS_DEFAULT</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">\\\&#39;{ ... your local databricks connection ... }\\\&#39;</span>
<span class="w">  </span><span class="nt">volumes</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">./dags:/opt/airflow/dags</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">./plugins:/opt/airflow/plugins</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">./logs:/opt/airflow/logs</span>
<span class="w">  </span><span class="nt">depends_on</span><span class="p">:</span>
<span class="w">    </span><span class="nt">postgres</span><span class="p">:</span>
<span class="w">      </span><span class="nt">condition</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">service_healthy</span>
<span class="w">    </span><span class="nt">airflow-init</span><span class="p">:</span>
<span class="w">      </span><span class="nt">condition</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">service_completed_successfully</span>

<span class="nt">services</span><span class="p">:</span>
<span class="w">  </span><span class="nt">postgres</span><span class="p">:</span>
<span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">postgres:13</span>
<span class="w">    </span><span class="c1"># ... postgres config ...</span>
<span class="w">  </span><span class="nt">airflow-init</span><span class="p">:</span>
<span class="w">    </span><span class="nt">&lt;&lt;</span><span class="p">:</span><span class="w"> </span><span class="nv">*airflow-common</span>
<span class="w">    </span><span class="nt">container_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">airflow-init</span>
<span class="w">    </span><span class="nt">entrypoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/bin/bash</span>
<span class="w">    </span><span class="nt">command</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-c</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">|</span><span class="w"> </span><span class="c1"># Initializes DB and creates admin user</span>
<span class="w">        </span><span class="no">set -e; if [ ! -f &quot;/opt/airflow/airflow_db_initialized.flag&quot; ]; then</span>
<span class="w">          </span><span class="no">airflow db init;</span>
<span class="w">          </span><span class="no">airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true;</span>
<span class="w">          </span><span class="no">touch /opt/airflow/airflow_db_initialized.flag; fi;</span>
<span class="w">        </span><span class="no">airflow db upgrade;</span>
<span class="w">    </span><span class="nt">restart</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">on-failure</span>
<span class="w">  </span><span class="nt">airflow-webserver</span><span class="p">:</span>
<span class="w">    </span><span class="nt">&lt;&lt;</span><span class="p">:</span><span class="w"> </span><span class="nv">*airflow-common</span>
<span class="w">    </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">webserver</span>
<span class="w">    </span><span class="nt">ports</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;8080:8080&quot;</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="c1"># ... healthcheck &amp; restart ...</span>
<span class="w">  </span><span class="nt">airflow-scheduler</span><span class="p">:</span>
<span class="w">    </span><span class="nt">&lt;&lt;</span><span class="p">:</span><span class="w"> </span><span class="nv">*airflow-common</span>
<span class="w">    </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">scheduler</span>
<span class="w">    </span><span class="c1"># ... healthcheck &amp; restart ...</span>
<span class="nt">volumes</span><span class="p">:</span>
<span class="w">  </span><span class="nt">airflow-db-volume</span><span class="p">:</span><span class="w"> </span><span class="c1"># Persists Postgres data</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</section>
<section id="managing-environments-test-vs-prod">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Managing Environments (Test vs. Prod)</a><a class="headerlink" href="#managing-environments-test-vs-prod" title="Link to this heading"></a></h2>
<ul>
<li><p><strong>Airflow Variables &amp; Connections</strong>:</p>
<ul>
<li><p>Define separate connections for test and prod Databricks workspaces: e.g., <code class="docutils literal notranslate"><span class="pre">databricks_test</span></code>, <code class="docutils literal notranslate"><span class="pre">databricks_prod</span></code>.</p></li>
<li><p>Use Airflow Variables for environment-specific parameters (cluster IDs, paths, instance pool IDs).</p></li>
<li><p><em>Code Example (Accessing variables in DAG)</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">airflow.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">Variable</span>

<span class="n">databricks_conn_id</span> <span class="o">=</span> <span class="n">Variable</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;databricks_conn_id&quot;</span><span class="p">,</span> <span class="n">default_var</span><span class="o">=</span><span class="s2">&quot;databricks_test&quot;</span><span class="p">)</span>
<span class="n">target_cluster_id</span> <span class="o">=</span> <span class="n">Variable</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;databricks_target_cluster_id_prod&quot;</span> <span class="k">if</span> <span class="n">databricks_conn_id</span> <span class="o">==</span> <span class="s2">&quot;databricks_prod&quot;</span> <span class="k">else</span> <span class="s2">&quot;databricks_target_cluster_id_test&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p><strong>Branching Strategy</strong>:</p>
<ul class="simple">
<li><p>E.g., <code class="docutils literal notranslate"><span class="pre">develop</span></code> branch for test environment, <code class="docutils literal notranslate"><span class="pre">main</span></code> branch for production.</p></li>
<li><p>Changes are merged from <code class="docutils literal notranslate"><span class="pre">develop</span></code> to <code class="docutils literal notranslate"><span class="pre">main</span></code> after successful testing.</p></li>
</ul>
</li>
<li><p><strong>CI/CD</strong>:</p>
<ul class="simple">
<li><p>Automate deployment of DAGs and <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> to MWAA's S3 bucket upon merges to <code class="docutils literal notranslate"><span class="pre">main</span></code>.</p></li>
<li><p>Automate testing (DAG validation, unit tests) in CI pipeline.</p></li>
</ul>
</li>
<li><p><strong>DAG Parameterization</strong>: Design DAGs to dynamically pick up configurations based on the environment (e.g., by checking an Airflow Variable like <code class="docutils literal notranslate"><span class="pre">environment=prod</span></code>).</p></li>
</ul>
</section>
<section id="potential-tradeoffs-and-considerations">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">Potential Tradeoffs and Considerations</a><a class="headerlink" href="#potential-tradeoffs-and-considerations" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Monorepo vs. Polyrepo</strong>:</p>
<ul>
<li><p><strong>Monorepo (Airflow + dbt + Notebooks in one repo)</strong>:</p>
<ul>
<li><p><em>Pros</em>: Simplified dependency management if shared, easier to coordinate changes.</p></li>
<li><p><em>Cons</em>: Can become large, tighter coupling, build/CI times might increase.</p></li>
</ul>
</li>
<li><p><strong>Polyrepo (Separate repos for Airflow, dbt, etc.)</strong>:</p>
<ul>
<li><p><em>Pros</em>: Clear ownership, independent release cycles, focused CI/CD.</p></li>
<li><p><em>Cons</em>: More complex to manage cross-repo dependencies and coordination.</p></li>
</ul>
</li>
<li><p><em>Recommendation</em>: Start with a monorepo for simplicity if the team is small and projects are tightly linked. Consider polyrepo as complexity grows.</p></li>
</ul>
</li>
<li><p><strong>Complexity of Local Setup vs. MWAA</strong>:</p>
<ul>
<li><p>Strive for similarity, but exact replication can be hard (e.g., MWAA's specific execution environment).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">uv</span></code> helps by producing a standard <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> that <code class="docutils literal notranslate"><span class="pre">pip</span></code> (used by MWAA) understands.</p></li>
</ul>
</li>
<li><p><strong>Databricks Operator Choices</strong>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">DatabricksSubmitRunOperator</span></code>: Submits a new one-time run. Good for dynamic tasks.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DatabricksRunNowOperator</span></code>: Triggers an existing Databricks job. Good if jobs are pre-defined in Databricks UI/API.</p></li>
<li><p>Consider how job definitions are managed (Airflow-defined vs. Databricks-defined).</p></li>
</ul>
</li>
<li><p><strong>dbt Integration Strategy</strong>:</p>
<ul>
<li><p>Running dbt via <code class="docutils literal notranslate"><span class="pre">DatabricksSubmitRunOperator</span></code> (as shown in the template's <code class="docutils literal notranslate"><span class="pre">dag_dbt_daily_models.py</span></code>):</p>
<ul>
<li><p><em>Pros</em>: Leverages Databricks compute, can handle large dbt projects, keeps dbt execution close to data.</p></li>
<li><p><em>Cons</em>: Requires managing the dbt project on Databricks (Repos/DBFS), a runner script, and ensuring dbt is installed on the cluster.</p></li>
</ul>
</li>
<li><p>Using Airflow dbt providers (e.g., <code class="docutils literal notranslate"><span class="pre">astronomer-cosmos</span></code>, <code class="docutils literal notranslate"><span class="pre">airflow-dbt-python</span></code>):</p>
<ul>
<li><p><em>Pros</em>: Can simplify DAG authoring, potentially better Airflow UI integration for dbt tasks.</p></li>
<li><p><em>Cons</em>: Adds another layer of abstraction, might have different dependency or execution model.</p></li>
</ul>
</li>
<li><p>Tradeoffs involve cost, execution environment, ease of debugging, and dependency management for dbt itself. The template's approach gives more control over the dbt execution environment on Databricks.</p></li>
</ul>
</li>
<li><p><strong>Dependency Management with ``uv`` and MWAA</strong>:</p>
<ul>
<li><p>MWAA uses <code class="docutils literal notranslate"><span class="pre">pip</span></code> with a <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code>. <code class="docutils literal notranslate"><span class="pre">uv</span></code> is used locally to <em>generate</em> this <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code>.</p></li>
<li><p>Ensure Python versions are compatible between local Docker and MWAA environment.</p></li>
</ul>
</li>
<li><p><strong>Testing Strategy</strong>:</p>
<ul>
<li><p>DAG validation tests (<code class="docutils literal notranslate"><span class="pre">airflow</span> <span class="pre">dags</span> <span class="pre">test</span></code>).</p></li>
<li><p>Unit tests for custom operators/hooks.</p></li>
<li><p>Integration tests (running DAGs against a test Databricks environment) can be complex and costly but provide high confidence.</p></li>
</ul>
</li>
<li><p><strong>Cost Management</strong>:</p>
<ul>
<li><p>MWAA pricing.</p></li>
<li><p>Databricks cluster costs (interactive vs. job clusters, instance types, auto-scaling).</p></li>
<li><p>Optimize DAGs: avoid unnecessary task runs, use efficient cluster configurations.</p></li>
</ul>
</li>
</ul>
</section>
<section id="other-common-concerns">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Other Common Concerns</a><a class="headerlink" href="#other-common-concerns" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Security</strong>:</p>
<ul>
<li><p>Secrets Management: Use AWS Secrets Manager for sensitive data (like Databricks tokens) and integrate with Airflow Connections.</p></li>
<li><p>IAM Permissions: Follow the principle of least privilege for MWAA execution role and Databricks service principals/users.</p></li>
<li><p>Network Configuration: MWAA VPC setup, security groups, private networking to Databricks if needed.</p></li>
</ul>
</li>
<li><p><strong>Monitoring and Alerting</strong>:</p>
<ul>
<li><p>Airflow UI for DAG status.</p></li>
<li><p>AWS CloudWatch for MWAA logs, metrics, and alarms.</p></li>
<li><p>Configure Airflow alerts (e.g., on task failure) via email, Slack (e.g., <code class="docutils literal notranslate"><span class="pre">SlackAPIPostOperator</span></code>).</p></li>
</ul>
</li>
<li><p><strong>Scalability</strong>:</p>
<ul>
<li><p>MWAA Environment Sizing: Choose appropriate instance class for MWAA.</p></li>
<li><p>Databricks Cluster Optimization: Right-size clusters, use instance pools, auto-scaling.</p></li>
<li><p>DAG Design: Maximize parallelism, avoid bottlenecks.</p></li>
</ul>
</li>
<li><p><strong>Idempotency</strong>: Design tasks to be rerunnable without causing duplicate data or incorrect states.</p></li>
<li><p><strong>Backfills</strong>: Plan for how to run DAGs for historical periods. Airflow's <code class="docutils literal notranslate"><span class="pre">catchup=True</span></code> and manual triggering.</p></li>
<li><p><strong>DAG Versioning &amp; Promotion</strong>:</p>
<ul>
<li><p>Use Git for version control of DAGs.</p></li>
<li><p>Promotion through environments (Dev -&gt; Test -&gt; Prod) typically handled by CI/CD and S3 sync strategies for MWAA.</p></li>
</ul>
</li>
</ul>
</section>
<section id="conclusion">
<h2><a class="toc-backref" href="#id12" role="doc-backlink">Conclusion</a><a class="headerlink" href="#conclusion" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Recap of key elements: structured project (based on <cite>clouddatastack/aws-airflow-databricks</cite>), <code class="docutils literal notranslate"><span class="pre">uv</span></code> for dependencies, Docker for local dev, MWAA for deployment, robust Databricks integration.</p></li>
<li><p>Emphasis on the importance of a well-thought-out project structure and operational practices from the start.</p></li>
<li><p>Encouragement to adapt the provided template and guidelines to specific organizational needs and scale.</p></li>
</ul>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id13" role="doc-backlink">References</a><a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Template Project: <a class="reference external" href="https://github.com/clouddatastack/aws-airflow-databricks">clouddatastack/aws-airflow-databricks</a></p></li>
<li><p><a class="reference external" href="https://docs.aws.amazon.com/mwaa/latest/userguide/">AWS Managed Workflows for Apache Airflow (MWAA) Documentation</a></p></li>
<li><p><a class="reference external" href="https://airflow.apache.org/docs/apache-airflow-providers-databricks/stable/index.html">Airflow Databricks Provider Documentation</a></p></li>
<li><p><a class="reference external" href="https://github.com/astral-sh/uv">uv Documentation</a></p></li>
<li><p><a class="reference external" href="https://docs.getdbt.com/">dbt Documentation</a></p></li>
<li><p><a class="reference external" href="https://docs.databricks.com/">Databricks Documentation</a></p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <!-- Empty footer.html to remove the footer -->
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
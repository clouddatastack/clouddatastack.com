5.2. From Incremental Loads to a Daily Snapshot
===================================================

Many data integrations, especially with third-party APIs, provide data incrementally. Instead of getting a full dump of a source table every day, you receive only the records that were created or updated on that day. While this is efficient for data extraction, it's less convenient for analytics, where analysts often need a complete "snapshot" of the table as it looked on a specific day.

This article presents a robust dbt pattern to transform daily incremental data loads into a complete daily snapshot table. This approach gives you the best of both worlds: the efficiency of incremental loads and the analytical power of historical snapshots.

The Core Logic
---------------

The goal is to build a dbt model that, for any given day, represents the complete state of the source data. We achieve this with a macro that intelligently combines the latest incremental data with the previous day's snapshot.

- **For an incremental run (daily operation):** The model takes all new and updated records from today's load and combines them with all the *unchanged* records from yesterday's snapshot.
- **For a full refresh (initial setup or recovery):** The model scans the entire history of incremental loads, finds the absolute latest version of each record, and builds a snapshot from scratch.

The Macro
----------

Here is a generalized dbt macro that implements this logic. It's designed to be placed in your ``macros`` directory.

.. literalinclude:: code/macros/incremental_api_to_snapshot_staging.sql
   :language: jinja
   :caption: macros/incremental_api_to_snapshot_staging.sql

**How to Use It**

You would call this macro from your staging model, which should be materialized as an incremental table itself, partitioned by ``event_date``.

.. code-block:: sql+jinja

   .. code-block:: jinja
 
   -- models/staging/stage_api_snapshots.sql
   {{
     config(
       materialized='incremental',
       unique_key='record_id',
       partition_by={
         "field": "event_date",
         "data_type": "date"
       }
     )
   }}
 
   {{ incremental_api_to_snapshot_staging(source('api_source', 'daily_loads')) }}


Potential Challenges and Solutions
-----------------------------------

This pattern is powerful, but real-world data introduces complexities. Hereâ€™s how to handle them.

1. Data Quality and Deduplication
----------------------------------

**Problem:** The integrity of your snapshot depends entirely on the quality of your incremental loads. If the source API sends duplicate ``record_id`` values within a single day's load, it can break your model.

**Solution:**

- **Initial Load:** The ``row_number()`` window function in the macro's ``else`` block effectively deduplicates the historical data, ensuring that only the most recent version of each record is used for the initial snapshot.
- **Incremental Loads:** The primary defense is testing. You must add data tests to your source-configured dbt model to ensure the uniqueness of the primary key (e.g., ``record_id``) for each daily partition (``event_date``).

.. code-block:: yaml

   # models/sources/sources.yml
   version: 2
   sources:
     - name: api_source
       schema: raw_data
       tables:
         - name: daily_loads
           columns:
             - name: record_id
               tests:
                 - dbt_utils.unique_combination_of_columns:
                     combination_of_columns:
                       - event_date
                       - record_id

2. Schema Changes
------------------

**Problem:** APIs evolve. A field might be added, removed, or have its data type changed. A standard dbt incremental model can fail or, worse, silently ignore new columns if not configured correctly.

**Solution:**

- **Adding Columns:** The use of ``select *`` and ``select * except(...)`` makes this macro resilient to newly added columns. They will be automatically included in both the incremental and full-refresh paths.
- **Removing/Renaming/Changing Data Types:** These are breaking changes. The simplest solution is to run a full refresh: ``dbt run --full-refresh -s stage_api_snapshots``. This rebuilds the entire snapshot using the new schema from the source. For a more automated but complex solution, you could explore dbt's ``on_schema_change`` configuration in your staging model, though it requires careful implementation to work with this custom snapshot logic.

3. Late-Arriving Data
----------------------

**Problem:** What happens if data for a previous day arrives late? For example, on Friday, you receive some data that should have been included in Wednesday's load. The standard macro logic won't pick this up.

**Solution:**

This requires adjusting your dbt job's orchestration. Instead of processing only the current date, you can process a lookback window (e.g., the last 3 days).

- **Orchestration:** Your dbt job would run the model for each day in the lookback window. For example:
  - ``dbt run -s stage_api_snapshots --vars '{event_date: "2023-10-10"}'``
  - ``dbt run -s stage_api_snapshots --vars '{event_date: "2023-10-11"}'``
  - ``dbt run -s stage_api_snapshots --vars '{event_date: "2023-10-12"}'``

- **Macro Adjustment:** The macro's date logic needs to be parameterized using a variable (e.g., ``var('event_date')``), as shown in the example code. This allows the orchestration tool to control which day is being processed.

By running the model for a window, you ensure that any late-arriving data for Wednesday is correctly processed and backfilled, and the changes are carried forward into the snapshots for Thursday and Friday.